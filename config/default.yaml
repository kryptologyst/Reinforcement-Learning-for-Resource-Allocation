# Default configuration for RL resource allocation project

environment:
  env_type: "standard"  # standard, simple, complex
  num_jobs: 3
  total_resources: 100
  max_steps: 10
  reward_type: "matching"  # matching, efficiency, fairness, combined
  noise_level: 0.1
  seed: 42

agent:
  type: "ppo"  # ppo, sac, td3, policy_gradient
  learning_rate: 0.0003
  gamma: 0.99
  batch_size: 64
  buffer_size: 100000
  hidden_dims: [64, 64]
  
  # PPO specific
  n_steps: 2048
  n_epochs: 10
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.0
  vf_coef: 0.5
  max_grad_norm: 0.5
  
  # SAC specific
  tau: 0.005
  train_freq: 1
  gradient_steps: 1
  ent_coef: "auto"
  target_update_interval: 1
  target_entropy: "auto"
  
  # TD3 specific
  target_policy_noise: 0.2
  target_noise_clip: 0.5
  policy_delay: 2

training:
  total_timesteps: 10000
  eval_freq: 1000
  save_freq: 5000
  log_freq: 100

logging:
  level: "INFO"
  tensorboard: true
  wandb: false
  wandb_project: "rl-resource-allocation"

paths:
  models: "models/"
  logs: "logs/"
  checkpoints: "checkpoints/"
